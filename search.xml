<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>问卷调查——新闻主题事件对社会稳定程度的影响</title>
      <link href="/questionaire/"/>
      <url>/questionaire/</url>
      <content type="html"><![CDATA[<p><link rel="stylesheet" href="https://cdn.bootcss.com/bootstrap/3.3.7/css/bootstrap.min.css"></p><p><link rel="stylesheet" href="/css/input.css"></p><script type="text/javascript" src="/js/src/clipboard.js"></script><h4 id="本人是来自中科院地理所的在读硕士研究生，目前正在撰写研究论文，该论文利用互联网新闻数据研究了“一带一路”沿线部分国家的社会发展态势，并对沿线国家的社会稳定程度进行评估，为确定社会稳定度的评估指标权重，特此进行问卷调研，谢谢各位老师、同学、朋友们的帮助与支持"><a href="#本人是来自中科院地理所的在读硕士研究生，目前正在撰写研究论文，该论文利用互联网新闻数据研究了“一带一路”沿线部分国家的社会发展态势，并对沿线国家的社会稳定程度进行评估，为确定社会稳定度的评估指标权重，特此进行问卷调研，谢谢各位老师、同学、朋友们的帮助与支持" class="headerlink" title="本人是来自中科院地理所的在读硕士研究生，目前正在撰写研究论文，该论文利用互联网新闻数据研究了“一带一路”沿线部分国家的社会发展态势，并对沿线国家的社会稳定程度进行评估，为确定社会稳定度的评估指标权重，特此进行问卷调研，谢谢各位老师、同学、朋友们的帮助与支持!"></a>本人是来自中科院地理所的在读硕士研究生，目前正在撰写研究论文，该论文利用互联网新闻数据研究了“一带一路”沿线部分国家的社会发展态势，并对沿线国家的社会稳定程度进行评估，为确定社会稳定度的评估指标权重，特此进行问卷调研，谢谢各位老师、同学、朋友们的帮助与支持!</h4><h4 id="由于本问卷中部分词汇涉及敏感词，若使用问卷星等常见的问卷工具，问卷会被封锁，因此在此博客网站上编写了调查问卷。"><a href="#由于本问卷中部分词汇涉及敏感词，若使用问卷星等常见的问卷工具，问卷会被封锁，因此在此博客网站上编写了调查问卷。" class="headerlink" title="由于本问卷中部分词汇涉及敏感词，若使用问卷星等常见的问卷工具，问卷会被封锁，因此在此博客网站上编写了调查问卷。"></a>由于本问卷中部分词汇涉及敏感词，若使用问卷星等常见的问卷工具，问卷会被封锁，因此在此博客网站上编写了调查问卷。</h4><p>本问卷仅作科研用途，希望您能够认真作答。<br>本研究不会泄露您的隐私信息。</p><p><strong>新闻主题事件为通过主题挖掘方法(LDA)从互联网新闻中挖掘出来的新闻事件，反映了社会热点、社会矛盾与冲突。</strong></p><p>本问卷共设计19个问题，我们需要您根据经验与专业知识判断<strong>19类新闻主题事件反映社会稳定程度的能力大小</strong>，并为其赋予-10~10之间的值，负值代表不稳定，正值代表稳定，分值可以为小数。</p><blockquote><p>例如：我认为若一个国家的新闻中出现军事冲突事件，则某种程度上说明该国家社会稳定度较低，因此军事冲突事件反应社会稳定的能力为8,因其是负面影响，所以打分为-8。<br>若认为某一事件无法反应社会稳定程度，可将分值打为0左右。</p></blockquote><h4 id="非常感谢"><a href="#非常感谢" class="headerlink" title="非常感谢!!!"></a>非常感谢!!!</h4><a id="more"></a><ol><li>您认为“经济活动”对社会稳定的影响力为(-10~10):<input id="1" placeholder=""></li><li>您认为“能源活动”对社会稳定的影响力为(-10~10):<input id="2" placeholder=""></li><li>您认为“旅游业””对社会稳定的影响力为(-10~10):<input id="3" placeholder=""></li><li>您认为“政治外交”对社会稳定的影响力为(-10~10):<input id="4" placeholder=""></li><li>您认为“航空活动”对社会稳定的影响力为(-10~10):<input id="5" placeholder=""></li><li>您认为“航天活动”对社会稳定的影响力为(-10~10):<input id="6" placeholder=""></li><li>您认为“教育活动”对社会稳定的影响力为(-10~10):<input id="7" placeholder=""></li></ol><p><code>军事活动中不包括军事冲突、战争等事件，主要以军事演习、军事装备等军事相关活动为主</code></p><ol start="8"><li>您认为“军事活动”对社会稳定的影响力为(-10~10):<input id="8" placeholder=""></li><li>您认为“军事冲突”对社会稳定的影响力为(-10~10):<input id="9" placeholder=""></li><li>您认为“恐怖袭击”对社会稳定的影响力为(-10~10):<input id="10" placeholder=""></li></ol><p><code>核制裁为欧美各国对伊朗的经济方面的制裁</code></p><ol start="11"><li>您认为“核制裁”对社会稳定的影响力为(-10~10):<input id="11" placeholder=""></li><li>您认为“难民”对社会稳定的影响力为(-10~10):<input id="12" placeholder=""></li><li>您认为“人权侵犯”对社会稳定的影响力为(-10~10):<input id="13" placeholder=""></li><li>您认为“毒品犯罪”对社会稳定的影响力为(-10~10):<input id="14" placeholder=""></li></ol><p><code>治安事件为涉及到犯罪、监禁、审判的事件</code></p><ol start="15"><li>您认为“治安”对社会稳定的影响力为(-10~10):<input id="15" placeholder=""></li><li>您认为“海军活动”对社会稳定的影响力为(-10~10):<input id="16" placeholder=""></li><li>您认为“医疗健康”对社会稳定的影响力为(-10~10):<input id="17" placeholder=""></li><li>您认为“种族冲突”对社会稳定的影响力为(-10~10):<input id="18" placeholder=""></li><li>您认为“宗教活动”对社会稳定的影响力为(-10~10):<input id="19" placeholder=""></li></ol><h5 id="下面请填写您的部分个人信息，仅用于统计说明，不会泄露此信息"><a href="#下面请填写您的部分个人信息，仅用于统计说明，不会泄露此信息" class="headerlink" title="下面请填写您的部分个人信息，仅用于统计说明，不会泄露此信息"></a>下面请填写您的部分个人信息，仅用于统计说明，不会泄露此信息</h5><ol><li>学校：<input id="20" placeholder=""></li><li>院系：<input id="21" placeholder=""></li><li>性别：<input id="22" placeholder=""></li><li>年龄：<input id="23" placeholder=""></li><li>学历(在读研究生视为研究生，其他学位同理)：<input id="24" placeholder=""></li><li>职业(若为学生可填学生)：<input id="25" placeholder=""></li><li>职称(若无可不填)：<input id="26" placeholder=""></li></ol><button type="button" onclick="upload()" class="btn btn-primary">一键上传答案</button><h4 id="再次感谢"><a href="#再次感谢" class="headerlink" title="再次感谢!!!"></a>再次感谢!!!</h4>]]></content>
      
      
    </entry>
    
    <entry>
      <title>由《垫底辣妹》引发的回忆与思考</title>
      <link href="/birigirl/"/>
      <url>/birigirl/</url>
      <content type="html"><![CDATA[<p>看电影的时候仿佛看到了当年的自己<br><a id="more"></a></p><blockquote><p>女主沙耶加小时候不擅长与人沟通，在学校里没有朋友，看到其他同学三两作伴，心中暗生羡慕，在一次被霸凌之后，母亲被叫到学校，在与老师沟通的过程中，老师表示霸凌在哪个学校都是存在的，这也是对学生的一种历练，有助于其成长，母亲为此不惜转学。母亲告诉她在转学到明兰女子中学后，即使不学习，也会有自动升学式的高中大学可以上，这样沙耶加只需要做让自己开心的事就好。在进入明兰后，女主很快认识了三个“辣妹”，她们带着女主一起玩耍，带假睫毛，去舞厅，抽烟，唱歌，令其变得“辣”，沙耶加很开心，但是很快，班主任在其包里发现一包烟，校长威胁她说出其他抽烟的人，否则就要令其退学，这个场景相信国内上学的很多人都有过相似的经验，确实很难抉择，当初的我义气为先，加以那个年龄的单纯，做出了与女主相同的选择，对，就是保持沉默。幸运的是，我仅仅是被计以象征性的大过处分，并未对我产生实质性的伤害，唯一的后果只是在后来的两年半的时间中，不得不很乖的遵守校级，避免再次被记过以免被退学。而女主则被处以无期休学处分。在这次事件中，母亲再次被叫到学校，这次母亲质问校长，为了自己而出卖自己的朋友，这就是这所学校的方针吗，若果真如此，那么我女儿宁愿退学，并表示为女儿的沉默表示骄傲。<br>看到这的时候，很是佩服这位母亲，有着孟母三迁式的理念，善于鼓励孩子，在出现问题的时候不是冲动的批评女儿，相反，这位母亲每次总是很冷静，站在女儿的角度思考问题，关心爱护女儿，这一点让我觉得这位母亲非常伟大，令人尊敬。在实际生活中，很多父母在呼吸了三四十年的空气后，往往会有“我吃过的盐比你吃过的饭都多，我走过的桥比你走过的路都多”想法，因此他们往往会忘记自己当年也是一个或单纯无知或活泼叛逆的小孩，在小孩出现问题的时候，不能做到换位思考，往往诉诸于冲动与暴力。<br>上面这段话是为了表达对沙耶加母亲的尊敬与赞美，以及对我的父母的感恩与赞美，如果没有他们，我也不会是现在的我。</p></blockquote><p>下面，回到女主身上，这个即将要参加高考的“垫底辣妹”。不得不说，女主是真好看，属于清纯可爱气质型美女，和“辣妹”还是不一样的。<br><img src="http://p81hctxgb.bkt.clouddn.com/syj1.png" alt="沙耶加与坪田老师的初见"><br>沙耶加在第一次测试中回答了所有题目，却一道都没对，坪田老师不得不说是因材施教的高手，在鼓励与夸奖沙耶加后，引导其定下考上庆应大学的目标。其实沙耶加在此之前，未曾对未来有过明确的规划，也未曾为自己设定如此明确的目标，未曾努力去学习过，每天就像草原上的骏马，漫无目的的寻欢作乐，类似于浑浑噩噩的状态。在开始补习后，坪田老师的教育方式很有趣，在轻松的氛围中引导沙耶加不断地去主动学习曾经荒废的知识，令人惊讶的是沙耶加是从小学的教材开始复习的。沙耶加与坪田老师打赌说如果她能够在暑假期间学完中学英语的话就给她看老师年轻时的长发照片。之后，沙耶加开启疯狂英语模式，吃饭、K歌、骑车路上的时候都会学习英语。看到这段就像是在看当初的自己，高三时每天晚上只休息五六个小时，尽可能多的挤出时间去复习、刷题，考研的时候每天按时作息，起床被单词，路上被单词，吃饭背单词，睡前背单词，用尽所有的碎片时间去复习，只有拼尽全力后才知道自己的极限在哪里，否则限制自己的不是天花板，而是自己的目标。<br>沙耶加成功的在三周时间内复习通过中学英语，看到了坪田老师在新西兰交换时拍摄的长发飘飘的帅照。初次证明了自己的实力。但对沙耶加来说，这只是个开始，她要面临的困难还要多得多，远远超出当时的我。她的英语阅读理解能力较差，日本史基本等于空白，把日本地图画成了日本国旗的样子，老师提问名古屋三英杰的时候，反问是黄段子吗？并且掰着手指说屁股、屁股、屁股(这段应该是和日语的谐音等有关)。可以说，她离庆应大学还要相当长的一段路要走，需要更多的努力与更大的决心才行。由于长时间的熬夜复习，沙耶加的睡眠时间只能调整为上课时间(恩，貌似我也是…)，被班主任暴力叫醒后，与其打了个赌，若沙耶加考上庆应大学，则班主任绕操场裸奔一圈，否则沙耶加裸奔一圈，很刺激。回家后又受到父亲的嘲讽，沙耶加的动力更足了，这时，为了赢得父亲的关心与尊重、赢得与老师的赌约、报答母亲的无私付出，沙耶加已经全力以赴。但在她觉得已经拼尽全力后，在两次月考中仍然未取得说的过去的成绩，女主濒临绝望，意志趋于消沉，失去了奋斗的动力。此时，家中又发生一些变故，弟弟对父亲的顺从终于到达极限，他退出棒球社，与小混混混在一起，消极的反抗着父亲的意志，家庭中的痛苦也达到顶峰。沙耶加在经过这些变故后，又拾起斗志，在参观庆应大学后，对其文化氛围和环境非常向往。于是重新回到补习班继续努力。<br>最终，功夫不负有心人，沙耶加的得偿所愿，成功考入庆应大学，沙耶加与父亲、沙耶加与弟弟、父亲与弟弟之间都互相实现了和解，家庭关系重新趋于和睦。结局比较美好，虽然是常用的电影手法，但整条奋斗轨迹确实是很有代表性的，能够令很多人找到共鸣，不论最终是否考入一志，但期间所付出的心血与全力以赴的充实是相同的，很多人终其一生，可能都不会再有那样的无知无畏、心无旁骛、全力以赴，所以，这也是那段时光如此被铭记被怀念的原因。<br>如今，非常感谢当年的自己能够那般努力，只是有点遗憾那时未曾给多年后的自己留下寄语。</p><p><strong>经典台词：</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">仅凭外表就判断我不行的大人，我一直都瞧不起他们。我什么都没有，这点我最清楚，如果没有目标，就不会被任何人期待。</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">如果你因心愿没实现，而正在心灰意冷，为宏大的目标而拼搏过的这段经历，将来一定会成为你的力量。所以，用你那总是满不在乎的笑脸，请继续挺起胸膛充满信心地走下去。这是我最喜欢你的地方。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">天不造人上人，亦不造人下人。</span><br></pre></td></tr></table></figure>]]></content>
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 电影 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>matplotlib设置中英文多种字体混合坐标轴名称</title>
      <link href="/matplotlib-multifonts-xlabel/"/>
      <url>/matplotlib-multifonts-xlabel/</url>
      <content type="html"><![CDATA[<p>在利用matplotlib绘图时，常常需要添加图例与坐标轴名称，而在论文中，对中英文有严格的不同的字体要求，但默认的方法中(如下所示)，无法分别为中文、英文指定不同字体(如下图)，因此需要借助其他方法。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> matplotlib.font_manager <span class="keyword">import</span> FontProperties</span><br><span class="line"><span class="comment"># 在python中，字体单位貌似是px，与pt间的换算关系为1pt=4/3px</span></span><br><span class="line">simsun = FontProperities(fname=<span class="string">r'C:\Windows\Fonts\simsun.ttc'</span>, size=<span class="number">10</span>) </span><br><span class="line">plt.xlabel(<span class="string">u'Di距离'</span>, fontproperties=simsun)</span><br></pre></td></tr></table></figure></p><p><img src="http://p81hctxgb.bkt.clouddn.com/m2.PNG" alt="图1"></p><a id="more"></a><p>经过摸索后发现<em>text</em>函数能够在指定位置按照指定字体与大小显示文字，因此可以通过调整文字位置来实现xlabel的功能并且能够按照指定字体分别显示中英文(如下图)。代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> pylab <span class="keyword">import</span> mpl, text</span><br><span class="line"><span class="keyword">from</span> matplotlib.font_manager <span class="keyword">import</span> FontProperties</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"></span><br><span class="line"><span class="comment"># windows下matplotlib显示中文一般有问题，需要专门进行设置才能正常显示</span></span><br><span class="line"><span class="comment"># 下面是两种设置的方式，使用时的参数也略有区别，在此不具体介绍</span></span><br><span class="line"><span class="comment"># 但需提前设置好，使得能够显示中文，否则中文字符位置显示为方框</span></span><br><span class="line">simsun = FontProperties(fname=<span class="string">r'C:\Windows\Fonts\simsun.ttc'</span>, size=<span class="number">10</span>) <span class="comment"># 宋体</span></span><br><span class="line">roman = FontProperties(fname=<span class="string">r'C:\Windows\Fonts\times.ttf'</span>, size=<span class="number">10</span>) <span class="comment"># Times new roman</span></span><br><span class="line">mpl.rcParams[<span class="string">'font.sans-serif'</span>] = [<span class="string">'SimSun'</span>]</span><br><span class="line">fontcn = &#123;<span class="string">'family'</span>: <span class="string">'SimSun'</span>,<span class="string">'size'</span>: <span class="number">10</span>&#125; <span class="comment"># 1pt = 4/3px</span></span><br><span class="line">fonten = &#123;<span class="string">'family'</span>:<span class="string">'Times New Roman'</span>,<span class="string">'size'</span>: <span class="number">10</span>&#125;</span><br><span class="line"></span><br><span class="line">plt.figure(<span class="number">1</span>)</span><br><span class="line">ax1 = plt.subplot(<span class="number">121</span>)  <span class="comment"># 左图</span></span><br><span class="line">ax2 = plt.subplot(<span class="number">122</span>)  <span class="comment"># 右图</span></span><br><span class="line">plt.sca(ax1)</span><br><span class="line"></span><br><span class="line">plt.xlim(<span class="number">0</span>,<span class="number">150</span>)</span><br><span class="line">plt.ylim(<span class="number">0</span>,<span class="number">0.08</span>)</span><br><span class="line">plt.xticks(range(<span class="number">0</span>,<span class="number">160</span>,<span class="number">10</span>),rotation=<span class="number">0</span>)</span><br><span class="line"><span class="comment">#　设置斜体-Times New Roman字体</span></span><br><span class="line">text(<span class="number">60</span>, <span class="number">-0.01</span>, <span class="string">u'Di'</span>, style=<span class="string">'italic'</span>, fontdict=fonten) </span><br><span class="line">text(<span class="number">70</span>, <span class="number">-0.01</span>, <span class="string">u'距离'</span>, fontdict=fontcn)</span><br><span class="line">text(<span class="number">85</span>, <span class="number">-0.01</span>, <span class="string">u'(km)'</span>, fontdict=fonten)</span><br><span class="line">plt.ylabel(<span class="string">u"核密度"</span>, fontproperties=simsun)</span><br><span class="line">plt.title(<span class="string">u"a.核密度"</span>)</span><br><span class="line"></span><br><span class="line">plt.sca(ax2)</span><br><span class="line">plt.xlim(<span class="number">0</span>,<span class="number">150</span>)</span><br><span class="line">plt.ylim(<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">plt.xticks(range(<span class="number">0</span>,<span class="number">160</span>,<span class="number">10</span>),rotation=<span class="number">0</span>)</span><br><span class="line">plt.yticks(np.linspace(<span class="number">0</span>,<span class="number">1</span>,<span class="number">11</span>))</span><br><span class="line"></span><br><span class="line">text(<span class="number">60</span>, <span class="number">-0.13</span>, <span class="string">u'Di'</span>, style=<span class="string">'italic'</span>, fontdict=fonten)</span><br><span class="line">text(<span class="number">70</span>, <span class="number">-0.13</span>, <span class="string">u'距离'</span>, fontdict=fontcn)</span><br><span class="line">text(<span class="number">85</span>, <span class="number">-0.13</span>, <span class="string">u'(km)'</span>, fontdict=fonten)</span><br><span class="line">plt.ylabel(<span class="string">u"累积概率"</span>, fontproperties=simsun)</span><br><span class="line">plt.title(<span class="string">u"b.累积概率"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p><img src="http://p81hctxgb.bkt.clouddn.com/m3.PNG" alt="图2"></p><p>上述代码可直接运行，字体设置无误即可得到上图中的结果，但<code>Di距离(km)</code>可能会重叠或者空隙较大，这是正常情况，需要调整文字位置。<code>text</code>函数的前两个参数为其位置参数，都与其x轴、y轴坐标刻度有关，以&lt;<code>text(70, -0.01, u’距离’, fontdict=fontcn)</code>为例，70说明其水平方向位置在坐标轴70处，-0.01说明其垂直方向位置在-0.01刻度处，按照这个规则即可以多种字体显示多段文字。</p><hr><p>以上，欢迎留言交流～</p>]]></content>
      
      <categories>
          
          <category> 制图 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> matplotlib </tag>
            
            <tag> 绘图 </tag>
            
            <tag> 多字体 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>python下进行lda主题挖掘(三)——计算困惑度perplexity</title>
      <link href="/py-lda-perplexity/"/>
      <url>/py-lda-perplexity/</url>
      <content type="html"><![CDATA[<p><a href="http://maself.top/py-lda-preprocess/">python下进行lda主题挖掘(一)——预处理(英文)</a><br><a href="http://maself.top/py-lda-train/">python下进行lda主题挖掘(二)——利用gensim训练LDA模型</a><br><a href="http://maself.top/py-lda-perplexity/">python下进行lda主题挖掘(三)——计算困惑度perplexity</a></p><hr><p>本篇是我的LDA主题挖掘系列的第三篇，专门来介绍如何对训练好的LDA模型进行评价。</p><blockquote><p>训练好好LDA主题模型后，如何评价模型的好坏？能否直接将训练好的模型拿去应用？这是一个比较重要的问题，在对模型精度要求比较高的项目或科研中，需要对模型进行评价。一般来说，LDA模型的主题数量都是需要需要根据具体任务进行调整的，即要评价不同主题数的模型的困惑度来选择最优的那个模型。</p></blockquote><a id="more"></a><p>那么，困惑度是什么？<br><strong>1.LDA主题模型困惑度</strong></p><p>这部分参照：<a href="http://blog.csdn.net/mandy_joe/article/details/41514097" target="_blank" rel="noopener">LDA主题模型评估方法–Perplexity</a>，不过后面发现这篇文章<a href="http://blog.csdn.net/jiaqiang_ruan/article/details/77989459?locationNum=2&amp;fps=1" target="_blank" rel="noopener">Perplexity(困惑度)</a>感觉写的更好一点，两篇都是翻译的维基百科。<br>perplexity是一种信息理论的测量方法，b的perplexity值定义为基于b的熵的能量（b可以是一个概率分布，或者概率模型），通常用于概率模型的比较<br>wiki上列举了三种perplexity的计算：<br>1.1 概率分布的perplexity<br>公式： <img src="http://p81hctxgb.bkt.clouddn.com/t1.png" alt="perplexity公式1"><br>其中H(p)就是该概率分布的熵。当概率P的K平均分布的时候，带入上式可以得到P的perplexity值=K。<br>1.2 概率模型的perplexity<br>公式：<img src="http://p81hctxgb.bkt.clouddn.com/t2.png" alt="perplexity公式2"><br>公式中的Xi为测试局，可以是句子或者文本，N是测试集的大小（用来归一化），对于未知分布q，perplexity的值越小，说明模型越好。<br>指数部分也可以用交叉熵来计算，略过不表。<br>1.3单词的perplexity<br>perplexity经常用于语言模型的评估，物理意义是单词的编码大小。例如，如果在某个测试语句上，语言模型的perplexity值为2^190，说明该句子的编码需要190bits<br><strong>2.困惑度perplexity公式</strong><br>$$ perplexity =  e^ {\frac{ - ∑log(p(w))}{N}} $$<br>其中，<strong>p(w)</strong>是指的测试集中出现的每一个词的概率，具体到LDA的模型中就是$p(w)=∑z p(z|d)<em>p(w|z)$ (</em>z,d分别指训练过的主题和测试集的各篇文档*)。分母的N是测试集中出现的所有词，或者说是测试集的总长度，不排重。</p><p><strong>3.计算困惑度的代码</strong></p><blockquote><p>下述代码中加载的.dictionary(字典)、.mm(语料)、.model(模型)文件均为在<a href="http://maself.top/py-lda-train/">python下进行lda主题挖掘(二)——利用gensim训练LDA模型</a>中得到的结果，如果文件格式与我不同，说明调用的不是同一个包，代码无法直接使用，可参考代码逻辑，若是已按照该博客中的方法得到上述文件，可直接调用下述代码计算困惑度。<br>PS：将语料经过TFIDF训练模型后计算得到的困惑度要远大于直接进行训练的困惑度（在我这边是这样），应该是正常情况，不必惊慌。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#-*-coding:utf-8-*-</span></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">reload(sys)</span><br><span class="line">sys.setdefaultencoding(<span class="string">'utf-8'</span>)</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> gensim.corpora <span class="keyword">import</span> Dictionary</span><br><span class="line"><span class="keyword">from</span> gensim <span class="keyword">import</span> corpora, models</span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line">logging.basicConfig(format=<span class="string">'%(asctime)s : %(levelname)s : %(message)s : '</span>, level=logging.INFO)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">perplexity</span><span class="params">(ldamodel, testset, dictionary, size_dictionary, num_topics)</span>:</span></span><br><span class="line">    <span class="string">"""calculate the perplexity of a lda-model"""</span></span><br><span class="line">    <span class="comment"># dictionary : &#123;7822:'deferment', 1841:'circuitry',19202:'fabianism'...]</span></span><br><span class="line">    <span class="keyword">print</span> (<span class="string">'the info of this ldamodel: \n'</span>)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">'num of testset: %s; size_dictionary: %s; num of topics: %s'</span>%(len(testset), size_dictionary, num_topics))</span><br><span class="line">    prep = <span class="number">0.0</span></span><br><span class="line">    prob_doc_sum = <span class="number">0.0</span></span><br><span class="line">    topic_word_list = [] <span class="comment"># store the probablity of topic-word:[(u'business', 0.010020942661849608),(u'family', 0.0088027946271537413)...]</span></span><br><span class="line">    <span class="keyword">for</span> topic_id <span class="keyword">in</span> range(num_topics):</span><br><span class="line">        topic_word = ldamodel.show_topic(topic_id, size_dictionary)</span><br><span class="line">        dic = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> word, probability <span class="keyword">in</span> topic_word:</span><br><span class="line">            dic[word] = probability</span><br><span class="line">        topic_word_list.append(dic)</span><br><span class="line">    doc_topics_ist = [] <span class="comment">#store the doc-topic tuples:[(0, 0.0006211180124223594),(1, 0.0006211180124223594),...]</span></span><br><span class="line">    <span class="keyword">for</span> doc <span class="keyword">in</span> testset:</span><br><span class="line">        doc_topics_ist.append(ldamodel.get_document_topics(doc, minimum_probability=<span class="number">0</span>))</span><br><span class="line">    testset_word_num = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(testset)):</span><br><span class="line">        prob_doc = <span class="number">0.0</span> <span class="comment"># the probablity of the doc</span></span><br><span class="line">        doc = testset[i]</span><br><span class="line">        doc_word_num = <span class="number">0</span> <span class="comment"># the num of words in the doc</span></span><br><span class="line">        <span class="keyword">for</span> word_id, num <span class="keyword">in</span> doc:</span><br><span class="line">            prob_word = <span class="number">0.0</span> <span class="comment"># the probablity of the word </span></span><br><span class="line">            doc_word_num += num</span><br><span class="line">            word = dictionary[word_id]</span><br><span class="line">            <span class="keyword">for</span> topic_id <span class="keyword">in</span> range(num_topics):</span><br><span class="line">                <span class="comment"># cal p(w) : p(w) = sumz(p(z)*p(w|z))</span></span><br><span class="line">                prob_topic = doc_topics_ist[i][topic_id][<span class="number">1</span>]</span><br><span class="line">                prob_topic_word = topic_word_list[topic_id][word]</span><br><span class="line">                prob_word += prob_topic*prob_topic_word</span><br><span class="line">            prob_doc += math.log(prob_word) <span class="comment"># p(d) = sum(log(p(w)))</span></span><br><span class="line">        prob_doc_sum += prob_doc</span><br><span class="line">        testset_word_num += doc_word_num</span><br><span class="line">    prep = math.exp(-prob_doc_sum/testset_word_num) <span class="comment"># perplexity = exp(-sum(p(d)/sum(Nd))</span></span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"the perplexity of this ldamodel is : %s"</span>%prep)</span><br><span class="line">    <span class="keyword">return</span> prep</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    middatafolder = <span class="string">r'E:\work\lda'</span> + os.sep</span><br><span class="line">    dictionary_path = middatafolder + <span class="string">'dictionary.dictionary'</span></span><br><span class="line">    corpus_path = middatafolder + <span class="string">'corpus.mm'</span></span><br><span class="line">    ldamodel_path = middatafolder + <span class="string">'lda.model'</span></span><br><span class="line">    dictionary = corpora.Dictionary.load(dictionary_path)</span><br><span class="line">    corpus = corpora.MmCorpus(corpus_path)</span><br><span class="line">    lda_multi = models.ldamodel.LdaModel.load(ldamodel_path)</span><br><span class="line">    num_topics = <span class="number">50</span></span><br><span class="line">    testset = []</span><br><span class="line">    <span class="comment"># sample 1/300</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(corpus.num_docs/<span class="number">300</span>):</span><br><span class="line">        testset.append(corpus[i*<span class="number">300</span>])</span><br><span class="line">    prep = perplexity(lda_multi, testset, dictionary, len(dictionary.keys()), num_topics)</span><br></pre></td></tr></table></figure><p><strong>参考资料</strong></p><p>1.<a href="Perplexityblog.csdn.net/mandy_joe/article/details/41514097">LDA主题模型评估方法</a><br>2.<a href="http://blog.csdn.net/dongweionly/article/details/50286961" target="_blank" rel="noopener">LDA perplexity计算</a> java写的代码，本博客中的代码是参照改博客函数写的。<br>3.<a href="http://blog.csdn.net/jiaqiang_ruan/article/details/77989459?locationNum=2&amp;fps=1" target="_blank" rel="noopener">Perplexity(困惑度)</a></p><hr><p>以上，欢迎交流。</p>]]></content>
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> lda </tag>
            
            <tag> 自然语言处理 </tag>
            
            <tag> perplexity </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>python下进行lda主题挖掘(二)——利用gensim训练LDA模型</title>
      <link href="/py-lda-train/"/>
      <url>/py-lda-train/</url>
      <content type="html"><![CDATA[<p><a href="http://maself.top/py-lda-preprocess/">python下进行lda主题挖掘(一)——预处理(英文)</a><br><a href="http://maself.top/py-lda-train/">python下进行lda主题挖掘(二)——利用gensim训练LDA模型</a><br><a href="http://maself.top/py-lda-perplexity/">python下进行lda主题挖掘(三)——计算困惑度perplexity</a></p><hr><p>本篇是我的LDA主题挖掘系列的第二篇，介绍如何利用gensim包提供的方法来训练自己处理好的语料。<br><a id="more"></a><br>gensim提供了多种方法：<br><strong>速度较慢的：</strong><br>具体参数说明及使用方法请参照官网：<a href="https://radimrehurek.com/gensim/models/ldamodel.html" target="_blank" rel="noopener">models.ldamodel – Latent Dirichlet Allocation</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> gensim.models.ldamodel <span class="keyword">import</span> LdaModel</span><br><span class="line"><span class="comment"># 利用处理好的语料训练模型</span></span><br><span class="line">lda = LdaModel(corpus, num_topics=<span class="number">10</span>)</span><br><span class="line"><span class="comment"># 推断新文本的主题分布</span></span><br><span class="line">doc_lda = lda[doc_bow]</span><br><span class="line"><span class="comment"># 用新语料更新模型</span></span><br><span class="line">lda.update(other_corpus)</span><br></pre></td></tr></table></figure><p><strong>速度较快，使用多核心的：</strong><br>具体参数说明及使用方法请参照官网：<a href="https://radimrehurek.com/gensim/models/ldamulticore.html" target="_blank" rel="noopener">models.ldamulticore – parallelized Latent Dirichlet Allocation</a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> gensim <span class="keyword">import</span> corpora, models </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lda = LdaMulticore(corpus, id2word=id2word, num_topics=<span class="number">100</span>)  <span class="comment"># train model</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(lda[doc_bow]) <span class="comment"># get topic probability distribution for a document</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lda.update(corpus2) <span class="comment"># update the LDA model with additional documents</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(lda[doc_bow])</span><br></pre></td></tr></table></figure></p><p><strong>使用多进程对性能的提升：</strong></p><blockquote><p>Wall-clock performance on the English Wikipedia (2G corpus positions, 3.5M documents, 100K features, 0.54G non-zero entries in the final bag-of-words matrix), requesting 100 topics:<br>(Measured on this i7 server with 4 physical cores, so that optimal workers=3, one less than the number of cores.)</p></blockquote><table><thead><tr><th>algorithm</th><th style="text-align:center">training time</th></tr></thead><tbody><tr><td>LdaMulticore(workers=1)</td><td style="text-align:center">2h30m</td></tr><tr><td>LdaMulticore(workers=2)</td><td style="text-align:center">1h24m</td></tr><tr><td>LdaMulticore(workers=3)</td><td style="text-align:center">1h6m</td></tr><tr><td>oldLdaModel</td><td style="text-align:center">3h44m</td></tr><tr><td>simply iterating over input corpus = I/O overhead</td><td style="text-align:center">20m</td></tr></tbody></table><p><strong>workers的值需要比电脑的核心数小1</strong><br>本文代码使用多核心的方法。<br>有问题欢迎留言交流。<br>本文在将语料转化为corpus后，进行了如下操作：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tfidf = models.TfidfModel(corpus)</span><br><span class="line">corpusTfidf = tfidf[corpus]</span><br></pre></td></tr></table></figure></p><p>这一步是用来调整语料中不同词的词频，将那些在所有文档中都出现的高频词的词频降低，具体原理可参见阮一峰老师的系列博客：<a href="http://www.ruanyifeng.com/blog/2013/03/tf-idf.html" target="_blank" rel="noopener">TF-IDF与余弦相似性的应用（一）：自动提取关键词</a>，我经过这一步处理后，貌似效果提升不明显，而且这一步时间消耗较大，不建议采用。可直接将corpus作为训练数据传入lda模型中。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#-*-coding:utf-8-*-</span></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">reload(sys)</span><br><span class="line">sys.setdefaultencoding(<span class="string">'utf-8'</span>)</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> codecs</span><br><span class="line"><span class="keyword">from</span> gensim.corpora <span class="keyword">import</span> Dictionary</span><br><span class="line"><span class="keyword">from</span> gensim <span class="keyword">import</span> corpora, models</span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> platform</span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line">logging.basicConfig(format=<span class="string">'%(asctime)s : %(levelname)s : %(message)s : '</span>, level=logging.INFO)</span><br><span class="line"></span><br><span class="line">platform_info = platform.platform().lower()</span><br><span class="line"><span class="keyword">if</span> <span class="string">'windows'</span> <span class="keyword">in</span> platform_info:</span><br><span class="line">    code = <span class="string">'gbk'</span></span><br><span class="line"><span class="keyword">elif</span> <span class="string">'linux'</span> <span class="keyword">in</span> platform_info:</span><br><span class="line">    code = <span class="string">'utf-8'</span></span><br><span class="line">path = sys.path[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GLDA</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""docstring for GdeltGLDA"""</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, stopfile=None)</span>:</span></span><br><span class="line">        super(GLDA, self).__init__()</span><br><span class="line">        <span class="keyword">if</span> stopfile:</span><br><span class="line">            <span class="keyword">with</span> codecs.open(stopfile, <span class="string">'r'</span>, code) <span class="keyword">as</span> f:</span><br><span class="line">                self.stopword_list = f.read().split(<span class="string">' '</span>)</span><br><span class="line">            <span class="keyword">print</span> (<span class="string">'the num of stopwords is : %s'</span>%len(self.stopword_list))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.stopword_list = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">lda_train</span><span class="params">(self, num_topics, datafolder, middatafolder, dictionary_path= \</span></span></span><br><span class="line"><span class="function"><span class="params">    None, corpus_path=None, iterations=<span class="number">5000</span>, passes=<span class="number">1</span>, workers=<span class="number">3</span>)</span>:</span>       </span><br><span class="line">        time1 = datetime.now()</span><br><span class="line">        num_docs = <span class="number">0</span></span><br><span class="line">        doclist = []</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> corpus_path <span class="keyword">or</span> <span class="keyword">not</span> dictionary_path: <span class="comment"># 若无字典或无corpus，则读取预处理后的docword。一般第一次运行都需要读取，在后期调参时，可直接传入字典与corpus路径</span></span><br><span class="line">            <span class="keyword">for</span> filename <span class="keyword">in</span> os.listdir(datafolder): <span class="comment"># 读取datafolder下的语料</span></span><br><span class="line">                <span class="keyword">with</span> codecs.open(datafolder+filename, <span class="string">'r'</span>, code) <span class="keyword">as</span> source_file:</span><br><span class="line">                    <span class="keyword">for</span> line <span class="keyword">in</span> source_file:</span><br><span class="line">                        num_docs += <span class="number">1</span></span><br><span class="line">                        <span class="keyword">if</span> num_docs%<span class="number">100000</span>==<span class="number">0</span>:</span><br><span class="line">                            <span class="keyword">print</span> (<span class="string">'%s, %s'</span>%(filename, num_docs))</span><br><span class="line">                        <span class="comment">#doc = [word for word in doc if word not in self.stopword_list]</span></span><br><span class="line">                        doclist.append(line.split(<span class="string">' '</span>))</span><br><span class="line">                <span class="keyword">print</span> (<span class="string">'%s, %s'</span>%(filename, num_docs))</span><br><span class="line">        <span class="keyword">if</span> dictionary_path:</span><br><span class="line">            dictionary = corpora.Dictionary.load(dictionary_path) <span class="comment"># 加载字典</span></span><br><span class="line">        <span class="keyword">else</span>:            </span><br><span class="line">            <span class="comment">#构建词汇统计向量并保存</span></span><br><span class="line">            dictionary = corpora.Dictionary(doclist)</span><br><span class="line">            dictionary.save(middatafolder + <span class="string">'dictionary.dictionary'</span>)</span><br><span class="line">        <span class="keyword">if</span> corpus_path:</span><br><span class="line">            corpus = corpora.MmCorpus(corpus_path) <span class="comment"># 加载corpus</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            corpus = [dictionary.doc2bow(doc) <span class="keyword">for</span> doc <span class="keyword">in</span> doclist]</span><br><span class="line">            corpora.MmCorpus.serialize(middatafolder + <span class="string">'corpus.mm'</span>, corpus) <span class="comment"># 保存corpus</span></span><br><span class="line">        tfidf = models.TfidfModel(corpus)</span><br><span class="line">        corpusTfidf = tfidf[corpus]</span><br><span class="line">        time2 = datetime.now()</span><br><span class="line">        lda_multi = models.ldamulticore.LdaMulticore(corpus=corpusTfidf, id2word=dictionary, num_topics=num_topics, \</span><br><span class="line">            iterations=iterations, workers=workers, batch=<span class="keyword">True</span>, passes=passes) <span class="comment"># 开始训练</span></span><br><span class="line">        lda_multi.print_topics(num_topics, <span class="number">30</span>) <span class="comment"># 输出主题词矩阵</span></span><br><span class="line">        <span class="keyword">print</span> (<span class="string">'lda training time cost is : %s, all time cost is : %s '</span>%(datetime.now()-time2, datetime.now()-time1))</span><br><span class="line">        <span class="comment">#模型的保存/ 加载</span></span><br><span class="line">        lda_multi.save(middatafolder + <span class="string">'lda_tfidf_%s_%s.model'</span>%(<span class="number">2014</span>, num_topics, iterations)) <span class="comment"># 保存模型</span></span><br><span class="line">        <span class="comment"># lda = models.ldamodel.LdaModel.load('zhwiki_lda.model') # 加载模型</span></span><br><span class="line">        <span class="comment"># save the doc-topic-id</span></span><br><span class="line">        topic_id_file = codecs.open(middatafolder + <span class="string">'topic.json'</span>, <span class="string">'w'</span>, <span class="string">'utf-8'</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(num_docs):</span><br><span class="line">            topic_id = lda_multi[corpusTfidf[i]][<span class="number">0</span>][<span class="number">0</span>] <span class="comment"># 取概率最大的主题作为文本所属主题</span></span><br><span class="line">            topic_id_file.write(str(topic_id)+ <span class="string">' '</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    datafolder = path + os.sep + <span class="string">'docword'</span> + os.sep <span class="comment"># 预处理后的语料所在文件夹，函数会读取此文件夹下的所有语料文件</span></span><br><span class="line">    middatafolder = path + os.sep + <span class="string">'middata'</span> + os.sep</span><br><span class="line">    dictionary_path = middatafolder + <span class="string">'dictionary.dictionary'</span> <span class="comment"># 已处理好的字典，若无，则设置为False</span></span><br><span class="line">    corpus_path = middatafolder + <span class="string">'corpus.mm'</span> <span class="comment"># 对语料处理过后的corpus，若无，则设置为False</span></span><br><span class="line">    <span class="comment"># stopfile = path + os.sep + 'rest_stopwords.txt' # 新添加的停用词文件</span></span><br><span class="line">    num_topics = <span class="number">50</span></span><br><span class="line">    passes = <span class="number">2</span> <span class="comment"># 这个参数大概是将全部语料进行训练的次数，数值越大，参数更新越多，耗时更长</span></span><br><span class="line">    iterations = <span class="number">6000</span></span><br><span class="line">    workers = <span class="number">3</span> <span class="comment"># 相当于进程数</span></span><br><span class="line">    lda = GLDA()</span><br><span class="line">    lda.lda_train(num_topics, datafolder, middatafolder, dictionary_path=dictionary_path, corpus_path=corpus_path, iterations=iterations, passes=passes, workers=workers)</span><br></pre></td></tr></table></figure></p><p>在训练好模型后该如何对模型进行评价，以选取合适的参数？<br>可参照下一篇博客<a href="http://maself.top/py-lda-perplexity/">python下进行lda主题挖掘(三)——计算困惑度</a></p><hr><p>以上，欢迎交流与指正。</p>]]></content>
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> lda </tag>
            
            <tag> 自然语言处理 </tag>
            
            <tag> 主题挖掘 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>python下进行lda主题挖掘(一)——预处理(英文)</title>
      <link href="/py-lda-preprocess/"/>
      <url>/py-lda-preprocess/</url>
      <content type="html"><![CDATA[<p><a href="http://maself.top/py-lda-preprocess/">python下进行lda主题挖掘(一)——预处理(英文)</a><br><a href="http://maself.top/py-lda-train/">python下进行lda主题挖掘(二)——利用gensim训练LDA模型</a><br><a href="http://maself.top/py-lda-perplexity/">python下进行lda主题挖掘(三)——计算困惑度perplexity</a></p><hr><h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><p>本人打算将LDA这部分的内容写成一个系列，不涉及算法思想，只分享代码与使用经验，包括但不限于以下内容：英文文档的预处理、LDA主题提取、perplexity计算。<br>运行环境：python27，windows-Linux皆可<br>代码如有纰漏或不完善之处，欢迎批评指正。<br><a id="more"></a></p><h2 id="预处理"><a href="#预处理" class="headerlink" title="预处理"></a>预处理</h2><p>英文文档的预处理主要包括以下几部分的内容：</p><ul><li>单词分割(英文比较简单，中文就得借助于结巴分词之类的包了)</li><li>去除停用词</li><li>去除标点、数字</li><li>词形还原</li><li>词干提取</li><li>去除非英文单词的内容</li></ul><p>停用词表可以在网上下载，主要包括一些对语义无帮助的单词以及字母等。不过根据任务的不同最好建立适用于语料库的停用词表，提高结果的可解释性。<br> 词形还原可以将单词的不同时态或单复数进行统一，能够减少词汇表的大小，建议必须要有这一步；<br> 词干提取则是直接提取单词的词根，如busy、business都会提取为busi，这一步能够大量的减少词汇量，但是也会导致歧义，因为不同词义的单词可能对应于同一个词根，所以这一步需要酌情考虑。<br> 下面贴代码。写了一个预处理类，提供三种功能：</p><ul><li>处理文件夹下的所有文件</li><li>处理单个文件</li><li>多进程处理文件夹下的所有文件</li></ul><p>主要部分都有注释</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#-*-coding:utf-8-*-</span></span><br><span class="line"><span class="keyword">import</span> codecs</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> string</span><br><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line"><span class="keyword">from</span> nltk.stem <span class="keyword">import</span> WordNetLemmatizer </span><br><span class="line"><span class="keyword">from</span> nltk.stem.porter <span class="keyword">import</span> PorterStemmer</span><br><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> wordnet</span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> multiprocessing</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">reload(sys)</span><br><span class="line">sys.setdefaultencoding(<span class="string">'utf-8'</span>)</span><br><span class="line">path = sys.path[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NlpPreProcess</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""preprocess the html of gdelt-data</span></span><br><span class="line"><span class="string">    arg: str</span></span><br><span class="line"><span class="string">        filepath of the stoplist-file</span></span><br><span class="line"><span class="string">    function: </span></span><br><span class="line"><span class="string">        preprocess_folder(source_folder, dest_folder): preprocess all files in the source-folder, and save the results to dest-folder</span></span><br><span class="line"><span class="string">        preprocess_file(doc): preprocess a doc, delete the punctuation,digit,meanningless word</span></span><br><span class="line"><span class="string">        generate_dict_from_file(filename): generator to parse dict from json file</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt;nlp_preprocess = NlpPreProcess('')"""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, stopfile, downlist)</span>:</span></span><br><span class="line">        super(NlpPreProcess, self).__init__()</span><br><span class="line">        self.wnl = WordNetLemmatizer() <span class="comment"># 词形还原</span></span><br><span class="line">        self.ps = PorterStemmer() <span class="comment"># 词干提取</span></span><br><span class="line">        <span class="keyword">with</span> codecs.open(stopfile, <span class="string">'r'</span>, <span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">            self.stoplist = f.read().splitlines()</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">'the num of stopwords is %s'</span>%len(self.stoplist))</span><br><span class="line">        self.downlist = downlist <span class="comment"># 文件夹下已经处理过的文档，避免重复抓取</span></span><br><span class="line">        self.allnum = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">preprocess_folder</span><span class="params">(self, source_folder, dest_folder)</span>:</span></span><br><span class="line">        <span class="string">'''process all docs in all files, and save the results to according docwords file'''</span></span><br><span class="line">        stime = datetime.now()</span><br><span class="line">        <span class="keyword">for</span> filename <span class="keyword">in</span> os.listdir(source_folder):</span><br><span class="line">            self.preprocess_file(filename, source_folder, dest_folder)</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">'the num of all valid docs is : %s'</span>%self.allnum)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">preprocess_folder_multiprocess</span><span class="params">(self, source_folder, dest_folder, process_num)</span>:</span></span><br><span class="line">        <span class="string">'''process all docs in all files, and save the results to according docwords file'''</span></span><br><span class="line">        filelist = os.listdir(source_folder)</span><br><span class="line">        tmp = []</span><br><span class="line">        process_list = []</span><br><span class="line">        num = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(filelist)):</span><br><span class="line">            i += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> <span class="string">'docwords'</span>+filelist[i<span class="number">-1</span>] <span class="keyword">in</span> downlist: <span class="comment"># 如果已处理则跳过</span></span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            tmp.append(filelist[i<span class="number">-1</span>])</span><br><span class="line">            num += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> num%process_num == <span class="number">0</span> <span class="keyword">or</span> i==len(filelist):</span><br><span class="line">                <span class="keyword">print</span> (<span class="string">"the %dth loop"</span>%(num/process_num))</span><br><span class="line">                <span class="keyword">print</span> (<span class="string">'------'</span>*<span class="number">20</span>)</span><br><span class="line">                <span class="keyword">for</span> filename <span class="keyword">in</span> tmp:</span><br><span class="line">                    process_list.append(multiprocessing.Process(target=self.preprocess_file, args=(filename, source_folder, dest_folder)))</span><br><span class="line">                <span class="keyword">for</span> p <span class="keyword">in</span> process_list:</span><br><span class="line">                    p.start()</span><br><span class="line">                <span class="keyword">for</span> p <span class="keyword">in</span> process_list:</span><br><span class="line">                    p.join()</span><br><span class="line">                tmp = []</span><br><span class="line">                process_list = []</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">'the num of all valid docs is : %s'</span>%self.allnum)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">preprocess_file</span><span class="params">(self, filename, source_folder, dest_folder)</span>:</span></span><br><span class="line">        <span class="string">'''去标点, 去数字, 分割成单词, 词形还原'''</span></span><br><span class="line">        saveFileName = <span class="string">'docwords'</span>+filename[<span class="number">-1</span>]</span><br><span class="line">        <span class="keyword">if</span> saveFileName <span class="keyword">in</span> self.downlist:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        <span class="keyword">print</span> (<span class="string">'begin process %s'</span>%filename)</span><br><span class="line">        save_file = codecs.open(dest_folder + os.sep + saveFileName, <span class="string">'w'</span>, <span class="string">'utf-8'</span>)</span><br><span class="line">        num = <span class="number">0</span></span><br><span class="line">        stime = datetime.now()</span><br><span class="line">        <span class="keyword">for</span> dic <span class="keyword">in</span> self.generate_dict_from_file(source_folder + os.sep + filename):</span><br><span class="line">            doc = dic[<span class="string">'content'</span>].lower()</span><br><span class="line">            <span class="keyword">for</span> c <span class="keyword">in</span> string.punctuation: <span class="comment">#去标点</span></span><br><span class="line">                doc = doc.replace(c, <span class="string">' '</span>)</span><br><span class="line">            <span class="keyword">for</span> c <span class="keyword">in</span> string.digits: <span class="comment">#去数字</span></span><br><span class="line">                doc = doc.replace(c, <span class="string">''</span>)</span><br><span class="line">            doc = nltk.word_tokenize(doc) <span class="comment">#分割成单词</span></span><br><span class="line">            <span class="comment"># 只保留特定词性单词, 如名词</span></span><br><span class="line">            <span class="comment"># filter = nltk.pos_tag(doc)</span></span><br><span class="line">            <span class="comment"># doc = [w for w, pos in filter if pos.startswith("NN")]</span></span><br><span class="line">            cleanDoc = []</span><br><span class="line">            <span class="comment"># 只保留长度不小于3的单词,去除停用词,验证是否为英文单词(利用wordnet)</span></span><br><span class="line">            <span class="keyword">for</span> word <span class="keyword">in</span> doc:</span><br><span class="line">                <span class="keyword">if</span> len(word) &gt;= <span class="number">3</span> <span class="keyword">and</span> word <span class="keyword">not</span> <span class="keyword">in</span> self.stoplist <span class="keyword">and</span> wordnet.synsets(word):</span><br><span class="line">                    word = self.wnl.lemmatize(word) <span class="comment">#词形还原</span></span><br><span class="line">                    <span class="comment">#word = self.ps.stem(word) # 词干提取</span></span><br><span class="line">                    cleanDoc.append(word)</span><br><span class="line">            dic[<span class="string">'content'</span>] = <span class="string">' '</span>.join(cleanDoc)</span><br><span class="line">            json.dump(dic, save_file, ensure_ascii=<span class="keyword">False</span>)</span><br><span class="line">            save_file.write(<span class="string">'\n'</span>)</span><br><span class="line">            num += <span class="number">1</span></span><br><span class="line">        <span class="keyword">print</span> (<span class="string">'time cost is : %s'</span>%(datetime.now()-stime))</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">'the num of valid docs is : %s'</span>%num)</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">'---'</span>*<span class="number">20</span>)</span><br><span class="line">        self.allnum += num</span><br><span class="line">        <span class="keyword">return</span> num</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">generate_dict_from_file</span><span class="params">(self, filename)</span>:</span></span><br><span class="line">        <span class="string">"""读取json文件，返回字典数据"""</span></span><br><span class="line">        <span class="keyword">with</span> codecs.open(filename, <span class="string">'r'</span>, <span class="string">'utf-8'</span>) <span class="keyword">as</span> source_file:</span><br><span class="line">            <span class="keyword">for</span> line <span class="keyword">in</span> source_file:</span><br><span class="line">                <span class="keyword">try</span>:</span><br><span class="line">                    dic = json.loads(line.strip())</span><br><span class="line">                    <span class="keyword">yield</span> dic</span><br><span class="line">                <span class="keyword">except</span>:</span><br><span class="line">                    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    source_folder = path + os.sep + <span class="string">'cleanHtml'</span></span><br><span class="line">    dest_folder = path + os.sep + <span class="string">'docword'</span></span><br><span class="line">    stopword_filepath = path + os.sep + <span class="string">'stoplist.csv'</span></span><br><span class="line">    process_num = <span class="number">6</span> <span class="comment"># 设置多进程数量</span></span><br><span class="line">    downlist = os.listdir(dest_folder)</span><br><span class="line">    nlp_preprocess = NlpPreProcess(stopword_filepath, downlist)</span><br><span class="line">    nlp_preprocess.preprocess_file(<span class="string">'sogou.json'</span>, source_folder, dest_folder)</span><br><span class="line">    <span class="comment">#nlp_preprocess.preprocess_folder_multiprocess(source_folder, dest_folder, process_num)</span></span><br></pre></td></tr></table></figure>]]></content>
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> lda </tag>
            
            <tag> 自然语言处理 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>利用python将json数据转换为csv格式</title>
      <link href="/py-json2csv/"/>
      <url>/py-json2csv/</url>
      <content type="html"><![CDATA[<p>假设.json文件中存储的数据为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">"type"</span>: <span class="string">"Point"</span>, <span class="string">"link"</span>: <span class="string">"http://www.dianping.com/newhotel/22416995"</span>, <span class="string">"coordinates"</span>: [<span class="number">116.37256372996957</span>, <span class="number">40.39798447055443</span>], <span class="string">"category"</span>: <span class="string">"经济型"</span>, <span class="string">"name"</span>: <span class="string">"北京荷塘山庄"</span>, <span class="string">"count"</span>: <span class="string">"278"</span>, <span class="string">"address"</span>: <span class="string">"北京市怀柔区黄花城村安四路"</span>, <span class="string">"price"</span>: <span class="string">"380"</span>&#125;</span><br><span class="line">&#123;<span class="string">"type"</span>: <span class="string">"Point"</span>, <span class="string">"link"</span>: <span class="string">"http://www.dianping.com/newhotel/19717653"</span>, <span class="string">"coordinates"</span>: [<span class="number">116.56881588256466</span>, <span class="number">40.43310967948417</span>], <span class="string">"category"</span>: <span class="string">"经济型"</span>, <span class="string">"name"</span>: <span class="string">"慕田峪长城鱼师傅乡村酒店"</span>, <span class="string">"count"</span>: <span class="string">"89"</span>, <span class="string">"address"</span>: <span class="string">"北京市怀柔区渤海镇苇店村(慕田峪长城下3公里处，近怀黄路)"</span>, <span class="string">"price"</span>: <span class="string">"258"</span>&#125;</span><br></pre></td></tr></table></figure><p>现在需要将上面的这些数据存为csv格式，其中字典的keys为csv中的属性名称，字典的values为csv中属性对应的值。<br><a id="more"></a></p><hr><p>如果只需要按照json的keys来生成csv，那么操作比较简单，直接按照下面的方法即可：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#-*-coding:utf-8-*-</span></span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> codecs</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">trans</span><span class="params">(path)</span>:</span></span><br><span class="line">    jsonData = codecs.open(path+<span class="string">'.json'</span>, <span class="string">'r'</span>, <span class="string">'utf-8'</span>)</span><br><span class="line">    <span class="comment"># csvfile = open(path+'.csv', 'w') # 此处这样写会导致写出来的文件会有空行</span></span><br><span class="line">    <span class="comment"># csvfile = open(path+'.csv', 'wb') # python2下</span></span><br><span class="line">    csvfile = open(path+<span class="string">'.csv'</span>, <span class="string">'w'</span>, newline=<span class="string">''</span>) <span class="comment"># python3下</span></span><br><span class="line">    writer = csv.writer(csvfile, delimiter=<span class="string">'\t'</span>, quoting=csv.QUOTE_ALL)</span><br><span class="line">    flag = <span class="keyword">True</span></span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> jsonData:</span><br><span class="line">        dic = json.loads(line[<span class="number">0</span>:<span class="number">-1</span>])</span><br><span class="line">        <span class="keyword">if</span> flag:</span><br><span class="line">            <span class="comment"># 获取属性列表</span></span><br><span class="line">            keys = list(dic.keys())</span><br><span class="line">            <span class="keyword">print</span> (keys)</span><br><span class="line">            writer.writerow(keys) <span class="comment"># 将属性列表写入csv中</span></span><br><span class="line">            flag = <span class="keyword">False</span></span><br><span class="line">        <span class="comment"># 读取json数据的每一行，将values数据一次一行的写入csv中</span></span><br><span class="line">        writer.writerow(list(dic.values()))</span><br><span class="line">    jsonData.close()</span><br><span class="line">    csvfile.close()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    path=str(sys.argv[<span class="number">1</span>]) <span class="comment"># 获取path参数</span></span><br><span class="line">    <span class="keyword">print</span> (path)</span><br><span class="line">    trans(path)</span><br></pre></td></tr></table></figure><p>在python3下运行，命令行输入</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python C:\Users\MaMQ\Documents\jsonToCsv.py C:\Users\MaMQ\Documents\data\geoFood</span><br></pre></td></tr></table></figure><p>其中第三个参数为需要转换的文件的路径和其名称，将其后缀删除。运行文件后即可得到转换后的csv文件。</p><hr><p>如果需要对json文件中每个字典的key字段进行修改，比如需要将上面dict中的coordinate中的经纬度数据取出来存为x、y数据，则可以按照下面的方法（此方法还可以调整每个属性显示的顺序，效果更好一点）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> codecs</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">trans</span><span class="params">(path)</span>:</span></span><br><span class="line">    jsonData = codecs.open(path+<span class="string">'.json'</span>, <span class="string">'r'</span>, <span class="string">'utf-8'</span>)</span><br><span class="line">    <span class="comment"># csvfile = open(path+'.csv', 'w') # 此处这样写会导致写出来的文件会有空行</span></span><br><span class="line">    <span class="comment"># csvfile = open(path+'.csv', 'wb') # python2下</span></span><br><span class="line">    csvfile = open(path+<span class="string">'.csv'</span>, <span class="string">'w'</span>, newline=<span class="string">''</span>) <span class="comment"># python3下</span></span><br><span class="line">    writer = csv.writer(csvfile, delimiter=<span class="string">'\t'</span>, quoting=csv.QUOTE_ALL)</span><br><span class="line">    keys=[<span class="string">'id'</span>, <span class="string">'name'</span>, <span class="string">'category'</span>, <span class="string">'price'</span>, <span class="string">'count'</span>, <span class="string">'type'</span>, <span class="string">'address'</span>, <span class="string">'link'</span>, <span class="string">'x'</span>, <span class="string">'y'</span>]</span><br><span class="line">    writer.writerow(keys)</span><br><span class="line">    i = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> dic <span class="keyword">in</span> jsonData:</span><br><span class="line">        dic = json.loads(dic[<span class="number">0</span>:<span class="number">-1</span>])</span><br><span class="line">        x = dic[<span class="string">'coordinates'</span>][<span class="number">0</span>]</span><br><span class="line">        y = dic[<span class="string">'coordinates'</span>][<span class="number">1</span>]</span><br><span class="line">        writer.writerow([str(i),dic[<span class="string">'name'</span>],dic[<span class="string">'category'</span>],dic[<span class="string">'price'</span>],dic[<span class="string">'count'</span>],dic[<span class="string">'type'</span>],dic[<span class="string">'address'</span>],dic[<span class="string">'link'</span>],x,y])</span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line">    jsonData.close()</span><br><span class="line">    csvfile.close()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    path = str(sys.argv[<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">print</span> (path)</span><br><span class="line">    trans(path)</span><br></pre></td></tr></table></figure><p>运行方法同上。<br>json文件是我在大众点评抓取的数据，存储格式为utf-8。建议使用codecs包来读取json数据，可指定编码方式。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jsonData = codecs.open(path + <span class="string">'.json'</span>, <span class="string">'r'</span>, encoding=<span class="string">'utf-8'</span>)</span><br></pre></td></tr></table></figure><p>欢迎交流讨论。</p><hr><p>参考资料：<br><a href="http://blog.csdn.net/huitailang1991/article/details/54946528" target="_blank" rel="noopener">csv.writer写入文件有多余的空行</a></p>]]></content>
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> json </tag>
            
            <tag> csv </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>python下利用opencv提取surf特征并保存</title>
      <link href="/py-opencv-surf/"/>
      <url>/py-opencv-surf/</url>
      <content type="html"><![CDATA[<h2 id="一、算法背景介绍"><a href="#一、算法背景介绍" class="headerlink" title="一、算法背景介绍"></a>一、算法背景介绍</h2><p>Lowe于2000年提出了SIFT算法，并于2004年加以完善和改进，SIFT特征对图像旋转、平移、缩放、亮度变化能够保持良好的不变性，且其独特性好，信息量较为丰富，得到了广泛的应用，但其提取计算量较大，效率较低，因此Bay等人提出了SURF算法，在保证特征点数量的情况下，提高了效率。<br>SURF算法首先构建Hessian矩阵，然后构建尺度空间(SIFT算法则使用DOG)，其在构建图像金字塔时原始图像大小保持不变，只改变滤波器大小，然后精确定位特征点并确定其主方向，最后生成特征点描述子。<br>此外，SURF算法得到的特征向量维度为64，而SIFT得到的是128维向量。<br><a id="more"></a></p><h2 id="二、实现代码"><a href="#二、实现代码" class="headerlink" title="二、实现代码"></a>二、实现代码</h2><p>下述代码计算images_folder文件夹下的所有图片的SURF特征，然后将图片与特征向量其保存到指定文件夹<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#-*-coding:utf-8-*-</span></span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">path = sys.path[<span class="number">0</span>] + os.sep</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">feature_extract</span><span class="params">(images_folder, draw_folder)</span>:</span></span><br><span class="line">    featureSum = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> filename <span class="keyword">in</span> os.listdir(images_folder):</span><br><span class="line">        <span class="keyword">if</span> <span class="string">'.jpg'</span> <span class="keyword">in</span> filename:</span><br><span class="line">            filepath + images_folder + filename</span><br><span class="line">            drawpath = draw_folder + filename  </span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">continue</span>  </span><br><span class="line">        img = cv2.imread(filepath)</span><br><span class="line">        filename = filepath.split(os.sep)[<span class="number">-1</span>].split(<span class="string">'.'</span>)[<span class="number">0</span>]</span><br><span class="line">        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)</span><br><span class="line">        <span class="comment"># set Hessian threshold</span></span><br><span class="line">        detector = cv2.xfeatures2d.SURF_create(<span class="number">2000</span>)</span><br><span class="line">        <span class="comment"># find keypoints and descriptors directly</span></span><br><span class="line">        kps, des = detector.detectAndCompute(gray, <span class="keyword">None</span>)</span><br><span class="line">        img = cv2.drawKeypoints(image=img, outImage=img, keypoints=kps, \ </span><br><span class="line">            flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS, color=(<span class="number">255</span>, <span class="number">0</span>, <span class="number">0</span>))</span><br><span class="line">        feature_name = images_folder + <span class="string">'features%s%s.feature'</span>%(os.sep, filename)</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            np.savetxt(feature_name, des, fmt=<span class="string">'%.5e'</span>) <span class="comment"># 保存特征向量</span></span><br><span class="line">            <span class="comment"># feature = np.loadtxt(feature_folder + filename) # 加载特征向量</span></span><br><span class="line">            <span class="comment"># cv2.imwrite(drawpath, img) # 保存绘制了SURF特征的图片</span></span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        featureSum += len(kps)</span><br><span class="line">    <span class="keyword">print</span> featureSum</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    images_folder = path + <span class="string">'images'</span> + os.sep</span><br><span class="line">    draw_folder = path + <span class="string">'results'</span> + os.sep + <span class="string">'drawImages'</span> + os.sep</span><br><span class="line">    feature_extract(images_folder, draw_folder)</span><br></pre></td></tr></table></figure></p><p>以上，欢迎交流～</p>]]></content>
      
      <categories>
          
          <category> 图像处理 </category>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> surf特征 </tag>
            
            <tag> 特征提取与保存 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>python下进行hsv颜色空间量化</title>
      <link href="/py-hsv-quantilize/"/>
      <url>/py-hsv-quantilize/</url>
      <content type="html"><![CDATA[<p>由于工作需要，需要计算颜色直方图来提取颜色特征，但若不将颜色空间进行量化，则直方图矢量维数过高，不便于使用。但是看了<strong>opencv API</strong>后并未发现提供了相关函数能够在计算颜色直方图的同时进行量化，因此这部分功能只能自己实现。下面分为两个部分进行介绍：<br><a id="more"></a></p><h2 id="一、颜色空间量化表"><a href="#一、颜色空间量化表" class="headerlink" title="一、颜色空间量化表"></a>一、颜色空间量化表</h2><p>由于RGB模型不够直观，不符合人类视觉习惯，因此在进行颜色特征提取前，需要将照片从RGB颜色模型转换为更符合人类视觉的HSV模型。在提取颜色特征时，最常用的方法之一为颜色直方图法，但一张图片中出现的颜色一般特别多，导致直方图矢量的维数较高，因此需要对HSV空间进行量化。根据人眼对颜色的感知特性，采用较为常用的量化方法，即按照如下对应关系进行量化：<br><img src="http://p81hctxgb.bkt.clouddn.com/hsv.png" alt="HSV量化对照表"><br>基于上述量化表，将各颜色分量按照下述公式合成为72维一维矢量： $G = 9H + 3S + V$</p><h2 id="二、量化代码"><a href="#二、量化代码" class="headerlink" title="二、量化代码"></a>二、量化代码</h2><p>代码使用纯python写成，效率偏低，处理388*500像素的照片用时1.45秒。在quantilize函数中，未使用if-else判断语句，因此至少节省了1/3的时间。但这个速度显然是无法令人满意的，用C++效率应该会更高点。如果有人有更好的想法，欢迎在下方评论交流。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#-*-coding:utf-8-*-</span></span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">colors</span><span class="params">(imagepath)</span>:</span></span><br><span class="line">    img = cv2.imread(imagepath)</span><br><span class="line">    hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)</span><br><span class="line">    nhsv = np.zeros(hsv.shape[:<span class="number">2</span>], dtype=np.uint8)</span><br><span class="line">    t2 = datetime.now()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(hsv.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(hsv.shape[<span class="number">1</span>]):</span><br><span class="line">            nhsv[i][j] = quantilize(hsv[i][j])</span><br><span class="line">    <span class="keyword">print</span> datetime.now() - t2</span><br><span class="line">    hist = cv2.calcHist([nhsv], [<span class="number">0</span>], <span class="keyword">None</span>, [<span class="number">72</span>], [<span class="number">0</span>,<span class="number">72</span>]) <span class="comment"># 40x faster than np.histogramfaster than np.histogram</span></span><br><span class="line">    plt.plot(hist,color = <span class="string">'r'</span>)</span><br><span class="line">    plt.xlim([<span class="number">0</span>, <span class="number">72</span>])</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">quantilize</span><span class="params">(value)</span>:</span></span><br><span class="line">    <span class="string">'''hsv直方图量化</span></span><br><span class="line"><span class="string">    value : [21, 144, 23] h, s, v</span></span><br><span class="line"><span class="string">    opencv中，h-[0,180], s-[0,255], v-[0,255]</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># </span></span><br><span class="line">    value[<span class="number">0</span>] = value[<span class="number">0</span>] * <span class="number">2</span></span><br><span class="line">    hlist = [<span class="number">20</span>, <span class="number">40</span>, <span class="number">75</span>, <span class="number">155</span>, <span class="number">190</span>, <span class="number">270</span>, <span class="number">290</span>, <span class="number">316</span>, <span class="number">360</span>]</span><br><span class="line">    svlist = [<span class="number">21</span>, <span class="number">178</span>, <span class="number">255</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(hlist)):</span><br><span class="line">        <span class="keyword">if</span> value[<span class="number">0</span>] &lt;= hlist[i]:</span><br><span class="line">            h = i % <span class="number">8</span></span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(svlist)):</span><br><span class="line">        <span class="keyword">if</span> value[<span class="number">1</span>] &lt;= svlist[i]:</span><br><span class="line">            s = i</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(svlist)):</span><br><span class="line">        <span class="keyword">if</span> value[<span class="number">2</span>] &lt;= svlist[i]:</span><br><span class="line">            v = i</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">9</span> * h + <span class="number">3</span> * s + v</span><br></pre></td></tr></table></figure></p><p>以上，欢迎批评交流～<br>如果觉得不错，欢迎点赞～</p>]]></content>
      
      <categories>
          
          <category> 图像处理 </category>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> hsv量化 </tag>
            
            <tag> 颜色空间 </tag>
            
        </tags>
      
    </entry>
    
  
  
</search>
